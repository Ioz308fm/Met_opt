{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJIYYz8BoS07"
   },
   "source": [
    "# <center>Лабораторная работа 2.</center>\n",
    "## <center>Многомерная безусловная оптимизация</center>\n",
    "\n",
    "*Автор материала: к.т.н., доцент кафедры Фундаметальной информатики и оптимального управления ВолГУ Михаил Алексеевич Харитонов*\n",
    "\n",
    "**Цель работы:** Приобретение практических навыков реализации методов поиска безусловного экстремума в языке Python с использованием Jupyter Notebook.\n",
    "\n",
    "**Задание:** Заполните ответ в клетках (где написано \"Ваш код здесь\" или \"Ваш ответ здесь\"), ответьте на вопросы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDfDRGZ7_zd-"
   },
   "source": [
    "# Часть 1 Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En5AiGj8Ja2k"
   },
   "source": [
    "## 1.1 Метод нулевого порядка (метод Хука-Дживса)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2vXCeiu50BF"
   },
   "source": [
    "\n",
    "**Шаг 1.** Задать начальную точку $x^0$,число $\\varepsilon >0$ для остановки алгоритма, начальные величины шагов по координатным направлениям $\\Delta_1,\\ldots, \\Delta_n>\\varepsilon$, ускоряющий множитель       $\\lambda>0$, коэффициент уменьшения шага $\\alpha > 1$. Положить $y^1=x^0$, $i=1$, $k=0$.\n",
    "\n",
    "**Шаг 2.** Осуществить исследующий поиск по выбранному координатному направлению:\n",
    "\n",
    "  a) если  $f(y^i+\\Delta_i d_i)<f(y^i)$, шаг считается удачным. В этом случае следует положить $y^{i+1}=y^i+\\Delta_i d_i$ и перейти к шагу 3.\n",
    "\n",
    "  b) если в п. a шаг неудачен, то делается шаг в противоположном направлении. Если $f(y^i-\\Delta_i d_i)<f(y^i)$, шаг считается удачным. В этом случае следует положить  $y^{i+1}=y^i-\\Delta_i d_i$ и перейти  к шагу 3.\n",
    "\n",
    "  c) если в пп. a и b шаги неудачны, положить $y^{i+1}=y^i$.\n",
    "\n",
    "**Шаг 3.** Проверить условия:\n",
    "\n",
    "  a) если $i<n$, то положить $i=i+1$ и перейти к шагу 2 (продолжить исследующий поиск по оставшимся направлениям);\n",
    "  b) если $i=n$, проверить успешность исследующего поиска:\n",
    "\n",
    "   - если $f(y^{n+1})<f(x^k)$, перейти к шагу 4.\n",
    "\n",
    "   - если $f(y^{n+1})\\geq f(x^k)$, перейти к шагу 5.\n",
    "\n",
    "**Шаг 4.** Провести поиск по образцу. Положить $x^{k+1}=y^{n+1}$, $y^1=x^{k+1}+\\lambda(x^{k+1}-x^{k}),i=1,k=k+1$ и перейти к шагу 2.\n",
    "\n",
    "**Шаг 5.** Проверить условие окончания:\n",
    "\n",
    "  a) если все $\\Delta_i\\leq\\varepsilon_i$, то поиск закончить: $x^* \\cong x^k$.\n",
    "\n",
    "  b) для тех $i$, для которых $\\Delta_i>\\varepsilon_i$, уменьшить величину шага: $\\Delta_i=\\frac{\\Delta_i}{\\alpha}$ .\n",
    "            Подожить $y^1=x^k$, $x^{k+1}=x^k$, $k=k+1, i=1$ и перейти к шагу 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTio8eoeLVBH"
   },
   "source": [
    "### Задание 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVeM5Lk9La4-"
   },
   "source": [
    "1) Напишите функцию реализующую алгоритм поиска минимима функции методом метод Хужа-Дживса.  \n",
    "2) С помощью функции найдите минимальное значение функции $$f_1(x)=x_1^3-x_1x_2+x^2_2-2x_1+3x_2-4.$$\n",
    "3) С помощью функции найдите минимальное значение функции Розенброка $$f_2(x)=(1-x_1)^{2}+100(x_2-x_1^{2})^{2}.$$\n",
    "\n",
    "4) С помощью функции найдите максимум функции $$f_3(x)=\\frac{10}{30(x_2-x_1^2)^2+5(1.5+x_1)^2+1}.$$\n",
    "\n",
    "5)  С помощью функции найдите минимальное значение функции  $$f_4(x)=x_1^2+5x_2^2+3x_3^2+4x_1x_2-2x_2x_3-2x_1x_3.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XwauhReQe3RM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def chucka(func, x0, step = 0.5,a = 2, eps = 1e-5):\n",
    "    points = [x0];\n",
    "    values = [func(x0)]\n",
    "    n = len(x0)\n",
    "\n",
    "    while step >= eps:\n",
    "        x_best = points[::-1][0].copy()\n",
    "        f_best = values[-1]\n",
    "        point = []\n",
    "\n",
    "        for i in range(n):\n",
    "            x_plus = x_best.copy()\n",
    "            x_plus[i] += step\n",
    "            f_plus = func(x_plus)\n",
    "\n",
    "            x_minus = x_best.copy()\n",
    "            x_minus[i] -= step\n",
    "            f_minus = func(x_minus)\n",
    "            if f_plus < f_best:\n",
    "                x_best, f_best = x_plus, f_plus\n",
    "            elif f_minus < f_best:\n",
    "                x_best, f_best = x_minus, f_minus\n",
    "            point.append(x_best[i])\n",
    "        if func(point) < values[-1]:\n",
    "            points.append(point)\n",
    "            values.append(func(point))\n",
    "            while True:\n",
    "                last_two = points[-2:].copy()\n",
    "                point = [x1 + (x1 - x0) for x0, x1 in zip(last_two[0], last_two[1])]\n",
    "                if func(point) < values[-1]:\n",
    "                    points.append(point)\n",
    "                    values.append(func(point))\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            step /= a\n",
    "    return \"x: \" + str([round(x, 3) for x in points[-1]]), \"f(x): \" + str(round(values[-1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZfH-taTni2E",
    "outputId": "e186e6f1-412d-418c-c7d9-5e3154952868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метод нулевого порядка (метод Хука-Дживса)\n",
      "\n",
      "f2:  ('x: [0.5, -1.25]', 'f(x): -6.438')\n",
      "f3:  ('x: [1.0, 1.0]', 'f(x): 0.0')\n"
     ]
    }
   ],
   "source": [
    "def f2(x) -> float:\n",
    "  return x[0]**3-x[0]*x[1]+x[1]**2-2*x[0]+3*x[1]-4\n",
    "\n",
    "def f3(x) -> float:\n",
    "  return (1-x[0])**2+100*(x[1]-x[0]**2)**2\n",
    "\n",
    "def f4(x) -> float:\n",
    "  return (10)/(30*(x[1]-x[0]**2)**2+5*(1.5+x[0])**2+1)\n",
    "\n",
    "def f5(x) -> float:\n",
    "  return x[0]**2+5*x[1]**2+3*x[2]**2+4*x[0]*x[1]-2*x[1]*x[2]-2*x[0]*x[2]\n",
    "\n",
    "\n",
    "print(\"Метод нулевого порядка (метод Хука-Дживса)\\n\")\n",
    "print(\"f2: \", chucka(f2, [5, 3]))\n",
    "print(\"f3: \", chucka(f3, [1.5, 1.2]))\n",
    "print(\"f4: \", chucka(f4, [-1.0, 0.0]))\n",
    "print(\"f5: \", chucka(f5, [1.7, -2.3, 5.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjuLXbxGN8M_"
   },
   "source": [
    "## 1.2 Метод первого порядка (метод градиентного спуска с постоянным шагом)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYGz2F0oOMp4"
   },
   "source": [
    "**Шаг 1.** Задать $x^0$, $0<\\varepsilon < 1$, $\\varepsilon_1 >0$, $\\varepsilon_2>0$, $M$ предельное число итераций.\n",
    "Найти градиент функции в произвольной точке $\\nabla f(x)=\\left ( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right)^T$\n",
    "\n",
    "**Шаг 2.** Положить $k=0$.\n",
    "\n",
    "**Шаг 3.** Вычислить $\\nabla f\\left(x^k\\right)$.\n",
    "\n",
    "**Шаг 4.** Проверить выполнение критерия окончания $\\left\\|\\nabla f\\left(x^k\\right)\\right\\|<\\varepsilon_1$:\n",
    "\n",
    "  - если критерий выполнен, расчет закончен, x^*=~x^k$;\n",
    "  - если критерий не выполнен, то перейти к шагу 5.\n",
    "\n",
    "\n",
    "**Шаг 5.** Проверить выполнение неравенства $k\\ge M$:\n",
    "\n",
    "  - если неравенство выполнено, то расчет окончен:  $x^*=~x^k$;\n",
    "  - если нет, то перейти к шагу 6.\n",
    "\n",
    "**Шаг 6.**  Задать величину шага $t_k$.\n",
    "\n",
    "**Шаг 7.**  Вычислить $x^{k+1}=~x^k-~t_k~\\nabla f\\left(x^k\\right)$.\n",
    "\n",
    "**Шаг 8.**  Проверить выполнение условия $f\\left(x^{k+1}\\right)-f\\left(x^k\\right)<0$ (или $f\\left(x^{k+1}\\right)-f\\left(x^k\\right)<-\\varepsilon \\left\\|\\nabla f(x^k)\\right\\|^2$):\n",
    "\n",
    "  - если условие выполнено, то перейти к шагу 9;\n",
    "  - если условие не выполнено, положить $t^k=~\\frac{t^k}{2}$ и перейти к шагу 7.\n",
    "\n",
    "\n",
    "**Шаг 9.** Проверить выполнение условий\n",
    "$\\|x^{k+1}-~x^k\\|<\\varepsilon_2$, $\\left\\|{f(x}^{k+1})-~f(x^k)\\right\\|<\\varepsilon_2$\n",
    "\n",
    "   - если оба условия выполнены при текущем значении $k$ и $k=~k-1$, то расчет окончен, $x^*=~x^{k+1}$;\n",
    "   - если хотя бы одно из условий не выполнено, положить $k=~k+1$ и перейти к шагу 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASJp7D2cQN8x"
   },
   "source": [
    "### Задание 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tumefd_hBnP7"
   },
   "source": [
    "a) градиентного спуска с постоянным шагом.\n",
    "\n",
    "б) наискорейшего градиентного спуска, т.е. $t_k = \\arg \\min\\limits_{t_k} f (x^k-t_k\\nabla f(x^k))$.\n",
    "\n",
    "в) [тяжелого шарика Поляка](http://mech.math.msu.su/~vvb/MasterAI/GradientDescent.html), т.е. $x^{k+1}=x^k−t_k\\nabla f(x^k)+\\beta (x^k−x^{k−1}), 0 < \\beta < 1$.\n",
    "\n",
    "г) [градиентного спуска Нестерова](http://mech.math.msu.su/~vvb/MasterAI/GradientDescent.html), т.е.    \n",
    "$x^{k+1}=x^k−t_k\\nabla f(y^k)$, $y^{k+1}=x^{k+1}+\\beta(x^{k+1}−x^k)$,  $0 < \\beta < 1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWc5_Ktoe15g"
   },
   "outputs": [],
   "source": [
    "#  Возвращает градиент функции в указанной точке\n",
    "def calculate_gradient(func, x, eps = 1e-6):\n",
    "    val = []\n",
    "    for i, k in enumerate(x):\n",
    "        delta_x = x.copy()\n",
    "        delta_x[i] += eps\n",
    "        val.append(round((func(delta_x) - func(x)) / eps, 4))\n",
    "    return val\n",
    "\n",
    "def gradient_descent_constant_step(func, x0, step = 0.5, eps1 = 1e-6, eps2 = 1e-6, max_iterations = 10000):\n",
    "    points = [x0]; values = [func(x0)]\n",
    "    for i in range(max_iterations):\n",
    "        old_p = points[-1:][0]; old_f = func(old_p)\n",
    "        gradient = calculate_gradient(func, old_p)\n",
    "        grad_value = math.sqrt(sum([x**2 for x in gradient]))\n",
    "        if grad_value < eps1:\n",
    "            break\n",
    "        while True:\n",
    "            new_p = [x - (step * y) for x, y in zip(old_p, gradient)]; new_f = func(new_p)\n",
    "            if new_f - old_f <= 0:\n",
    "                break\n",
    "            else:\n",
    "                step /= 2\n",
    "        norm = abs(math.sqrt(sum([x**2 for x in [x - y for x, y in zip(new_p, old_p)]])))\n",
    "        points.append(new_p)\n",
    "        values.append(func(new_p))\n",
    "        if norm < eps2 and abs(func(new_p) - func(old_p)) < eps2:\n",
    "            break\n",
    "    return \"x: \" + str([round(x, 3) for x in points[-1]]), \"f(x): \" +  str(round(values[-1], 3))\n",
    "\n",
    "def gradient_descent_fastest_step(func, x0, step = 0.5, eps1 = 1e-6, eps2 = 1e-6, max_iterations = 10000):\n",
    "    points = [x0]; values = [func(x0)]\n",
    "    for i in range(max_iterations):\n",
    "        old_p = points[-1:][0]; old_f = func(old_p)\n",
    "        gradient = calculate_gradient(func, old_p)\n",
    "        grad_value = math.sqrt(sum([x**2 for x in gradient]))\n",
    "        if grad_value < eps1:\n",
    "            break\n",
    "        step = t = 0.5\n",
    "        while step > eps1:\n",
    "            plus_f, curr_f, minus_f = [func([x - (t + step) * gr for x, gr in zip(old_p, gradient)])\n",
    "                                    for t in [t + step, t, t - step]]\n",
    "            new_p = [x - t * gr for x, gr in zip(old_p, gradient)]\n",
    "            if plus_f < curr_f and minus_f: t += step\n",
    "            elif minus_f < curr_f and plus_f: t -= step\n",
    "            else: step /= 2\n",
    "        norm = abs(math.sqrt(sum([x**2 for x in [x - y for x, y in zip(new_p, old_p)]])))\n",
    "        points.append(new_p)\n",
    "        values.append(func(new_p))\n",
    "        if norm < eps2 and abs(func(new_p) - old_f) < eps2:\n",
    "            break\n",
    "    return \"x: \" + str([round(x, 3) for x in points[-1]]), \"f(x): \" + str(round(values[-1], 3))\n",
    "\n",
    "def gradient_descent_ball_step(func, x0, step = 0.5, eps1 = 1e-6, eps2 = 1e-6, b = 0.25, max_iterations = 10000):\n",
    "    points = [x0]; values = [func(x0)]\n",
    "    for i in range(max_iterations):\n",
    "        old_p = points[-1:][0]; old_f = func(old_p)\n",
    "        gradient = calculate_gradient(func, old_p)\n",
    "        grad_value = math.sqrt(sum([x**2 for x in gradient]))\n",
    "        if grad_value < eps1:\n",
    "            break\n",
    "        old_p = points[-2:]\n",
    "        old_f, grad = func(old_p[-1]), calculate_gradient(func, old_p[-1])\n",
    "        step = 0.5\n",
    "        while True:\n",
    "            new_p = [x - step * y for x, y in zip(old_p[0], grad)] if len(points) == 1 else \\\n",
    "                    [xk1 - step * gr + b * (xk1 - xk0) for xk0, xk1, gr in zip(old_p[0], old_p[1], grad)]\n",
    "            new_f = func(new_p)\n",
    "            if new_f - old_f <= 0: break\n",
    "            else: step /= 2\n",
    "        norm = abs(math.sqrt(sum([x**2 for x in [x - y for x, y in zip(new_p, points[-1])]])))\n",
    "        points.append(new_p)\n",
    "        values.append(func(new_p))\n",
    "        if norm < eps2 and abs(func(new_p) - old_f) < eps2:\n",
    "            break\n",
    "    return \"x: \" + str([round(x, 3) for x in points[-1]]), \"f(x): \" + str(round(values[-1], 3))\n",
    "\n",
    "def gradient_descent_nester_step(func, x0, step = 0.5, eps1 = 1e-6, eps2 = 1e-6, b = 0.25, max_iterations = 10000):\n",
    "    x_points = [x0]; y_points = []; values = [func(x0)]\n",
    "    for i in range(max_iterations):\n",
    "        old_p = x_points[-1:][0]; old_f = func(old_p)\n",
    "        gradient = calculate_gradient(func, old_p)\n",
    "        grad_value = math.sqrt(sum([x**2 for x in gradient]))\n",
    "        if grad_value < eps1:\n",
    "            break\n",
    "        step = 0.5\n",
    "        old_p = x_points[-2:]; old_f = func(old_p[-1])\n",
    "        if len(y_points) == 0 : y_points.append(old_p[0])\n",
    "        grad = calculate_gradient(func, y_points[-1])\n",
    "        while True:\n",
    "            new_p = [y - step * gr for y, gr in zip(y_points[-1], grad)]\n",
    "            new_f = func(new_p)\n",
    "            if new_f - old_f < 0: break\n",
    "            else: step /= 2\n",
    "        y_points.append([xk1 + b * (xk1 - xk0) for xk0, xk1 in zip(old_p[-1], new_p)])\n",
    "\n",
    "        norm = abs(math.sqrt(sum([x**2 for x in [x - y for x, y in zip(new_p, x_points[-1])]])))\n",
    "        x_points.append(new_p)\n",
    "        values.append(func(new_p))\n",
    "        if norm < eps2 and abs(func(new_p) - old_f) < eps2:\n",
    "            break\n",
    "    return \"x: \" + str([round(x, 3) for x in x_points[-1]]), \"f(x): \" + str(round(values[-1], 3))\n",
    "\n",
    "print(\"а) Градиентного спуска с постоянным шагом\\n\")\n",
    "print(\"f2: \", gradient_descent_constant_step(f2, [1.2, 1.4]))\n",
    "print(\"f3: \", gradient_descent_constant_step(f3, [1.5, 1.2]))\n",
    "print(\"f4: \", gradient_descent_constant_step(f4, [-2.7, 2.27]))\n",
    "print(\"f5: \", gradient_descent_constant_step(f5, [-1.7, -2.3, 3.2]), \"\\n\")\n",
    "\n",
    "print(\"б) Наискорейшего градиентного спуска\\n\")\n",
    "print(\"f2: \", gradient_descent_fastest_step(f2, [1.2, 1.4]))\n",
    "print(\"f3: \", gradient_descent_fastest_step(f3, [1.5, 1.2]))\n",
    "print(\"f4: \", gradient_descent_fastest_step(f4, [-2.7, 2.27]))\n",
    "print(\"f5: \", gradient_descent_fastest_step(f5, [-1.7, -2.3, 3.2]), \"\\n\")\n",
    "\n",
    "print(\"в) Тяжелого шарика Поляка\\n\")\n",
    "print(\"f2: \", gradient_descent_ball_step(f2, [1.2, 1.4]))\n",
    "print(\"f3: \", gradient_descent_ball_step(f3, [1.2, 1.4]))\n",
    "print(\"f4: \", gradient_descent_ball_step(f4, [-2.7, 2.27], b=0.05))\n",
    "print(\"f5: \", gradient_descent_ball_step(f5, [0.0, 0.0, 0.0]), \"\\n\")\n",
    "\n",
    "print(\"г) Градиентного спуска Нестерова\\n\")\n",
    "print(\"f2: \", gradient_descent_nester_step(f2, [1.2, 1.4]))\n",
    "print(\"f3: \", gradient_descent_nester_step(f3, [1.5, 1.2]))\n",
    "print(\"f4: \", gradient_descent_nester_step(f4, [-2.7, 2.27]))\n",
    "print(\"f5: \", gradient_descent_nester_step(f5, [-1.7, -2.3, 3.2]), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBlMALt4Qdr-"
   },
   "source": [
    "# 1.3 Метод второго порядка (метод Ньютона)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HClLVLYwX1pp"
   },
   "source": [
    "**Шаг 1.** Задать $M, \\varepsilon_1>0,  \\varepsilon_2>0$ и точку начального приближения $x_0$;\n",
    "\n",
    "**Шаг 2.** Положить $k=0$.\n",
    "\n",
    "**Шаг 3.** Вычислить $\\nabla f(x_k)$.\n",
    "\n",
    "**Шаг 4.** Проверить выполнение критерия окончания $||\\nabla f(x_k)||\\leq \\varepsilon _1$:\n",
    "\n",
    "   - если неравествно выполнено, то расчет окончен и $x^*=x^k$;\n",
    "   - в противном случае перейти к шагу 5.\n",
    "\n",
    "\n",
    "**Шаг 5.** Проверить выполнение неравенства $k\\geq M$:\n",
    "\n",
    "   - если неравествно выполнено, то расчет окончен и $x^*=x^k$;\n",
    "   - в противном случае перейти к шагу 6.\n",
    "\n",
    "**Шаг 6.** Вычислить матрицу $H(x^k)$.\n",
    "\n",
    "**Шаг 7.** Вычислить матрицу $H^{-1}(x^k)$.\n",
    "\n",
    "**Шаг 8.** Проверить выполнение условия $H^{-1}(x^k)>0$:\n",
    "\n",
    "   - если $H^{-1}(x_k)>0$, то перейти к шагу 9;\n",
    "   - иначе перейти к шагу 10, положив $d^k=-\\nabla f(x^k)$.\n",
    "\n",
    "**Шаг 9.** Определить $d^k=-H^{-1}(x^k)\\nabla f(x^k)$.\n",
    "\n",
    "**Шаг 10.** Найти точку $x^{k+1}=x^k+t_kd^k$, положив $t_k=1$, если $d^k=-H^{-1}(x^k)\\nabla f(x^k)$, или выбрав $t_k$ из условия $f(x^{k+1})<f(x^k)$, если $d^k=-\\nabla f(x^k)$.\n",
    "\n",
    "**Шаг 11.** Проверить выполнение условий $||x^{k+1}-x^k||<\\varepsilon_1, |f(x^{k+1})-f(x^k)|<\\varepsilon_2$:\n",
    "   - если оба условия выполнены при текущем значение $k$ и $k=k-1$, то расчёт окончен, $x^*=x^{k+1}$;\n",
    "   - в противном случае положить $k=k+1$ и перейти к шагу 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki9CZIEkZZct"
   },
   "source": [
    "### Задание 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JokAJSoqZZ0Q"
   },
   "source": [
    "1) Напишите функцию реализующую алгоритм поиска минимима функции методом Ньютона.  \n",
    "2) С помощью функции найдите минимальное значение функции $$f_1(x)=x_1^3-x_1x_2+x^2_2-2x_1+3x_2-4.$$\n",
    "3) С помощью функции найдите минимальное значение функции Розенброка $$f_2(x)=(1-x_1)^{2}+100(x_2-x_1^{2})^{2}.$$\n",
    "\n",
    "4) С помощью функции найдите максимум функции $$f_3(x)=\\frac{10}{30(x_2-x_1^2)^2+5(1.5+x_1)^2+1}.$$\n",
    "\n",
    "5)  С помощью функции найдите минимальное значение функции  $$f_4(x)=x_1^2+5x_2^2+3x_3^2+4x_1x_2-2x_2x_3-2x_1x_3.$$\n",
    "\n",
    "**Бонусные баллы**\n",
    " - за собственную функцию вычисления $H(x^k)$.\n",
    " - проверку методов на [тестовых функциях](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D1%81%D1%82%D0%BE%D0%B2%D1%8B%D0%B5_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8_%D0%B4%D0%BB%D1%8F_%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwQLJC7nMPJ1"
   },
   "outputs": [],
   "source": [
    "def calculate_hessian(func, x, eps = 1.e-6):\n",
    "  hes = []; count = len(x)\n",
    "  for i in range(count):\n",
    "    hes.append([])\n",
    "    for j in range(count):\n",
    "        if i == j:\n",
    "            val = [func([a if k != i else a + eps for k, a in enumerate(x)]),\n",
    "                    func(x),\n",
    "                    func([a if k != i else a - eps for k, a in enumerate(x)])]\n",
    "            hes[i].append((val[0] - 2 * val[1] + val[2])/eps**2)\n",
    "        else:\n",
    "            val = [func([a if k != i and k != j else a + eps for k, a in enumerate(x)]),\n",
    "                    func([a if k != i else a + eps for k, a in enumerate(x)]),\n",
    "                    func([a if k != j else a + eps for k, a in enumerate(x)]),\n",
    "                    func(x)]\n",
    "            hes[i].append((val[0] -  val[1] - val[2] + val[3])/eps**2)\n",
    "  return hes\n",
    "\n",
    "def method_newton(func, x0, eps1 = 1e-6, eps2 = 1e-6, max_iterations = 100000):\n",
    "    points = [x0]; values = [func(x0)] # Список значений функции\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "      old_p = points[-1:][0]; old_f = func(old_p)\n",
    "      grad = calculate_gradient(func, old_p)\n",
    "      count = len(old_p)\n",
    "      hes = calculate_hessian(func, old_p)\n",
    "      d = [-1 * x for x in grad]\n",
    "      if np.linalg.det(hes) !=0 and np.linalg.det(np.linalg.inv(hes)) > 0:\n",
    "        d = np.dot(np.linalg.inv(hes), d)\n",
    "        new_p = [xk + dk for xk, dk in zip(old_p, d)]\n",
    "      else:\n",
    "        step = 0.5\n",
    "        while step > eps1:\n",
    "          new_p = [xk + step * dk for xk, dk in zip(old_p, d)]; new_f = func(new_p)\n",
    "          if new_f < old_f: break\n",
    "          else: step /= 2\n",
    "      norm = abs(math.sqrt(sum([x**2 for x in [x - y for x, y in zip(new_p, old_p)]])))\n",
    "      points.append(new_p)\n",
    "      values.append(func(new_p))\n",
    "      if norm < eps2 and abs(func(new_p) - old_f) < eps2:\n",
    "        break\n",
    "    return \"x: \" + str([round(x, 3) for x in points[-1]]), \"f(x): \" + str(round(values[-1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "box4YR01NYFV"
   },
   "outputs": [],
   "source": [
    "print(\"Метод второго порядка (метод Ньютона)\\n\")\n",
    "print(\"f2: \", method_newton(f2, [3.0, -2.0]))\n",
    "print(\"f3: \", method_newton(f3, [1.5, 1.2]))\n",
    "print(\"f4: \", method_newton(f4, [-1.5, 2.0]))\n",
    "print(\"f5: \", method_newton(f5, [-1.7, -2.3, 3.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsAUCmGIZjLZ"
   },
   "source": [
    "##Задание 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHJd7iFeZpLM"
   },
   "source": [
    "1) Выбрать функцию поиска минимума из библиотеки SciPy\n",
    " - опишите все входные и выходные параметры функции, типы данных параметров и методы оптимизации, которые применяются в функции\n",
    " - С помощью функции найдите минимальное значение функции $$f_1(x)=x_1^3-x_1x_2+x^2_2-2x_1+3x_2-4.$$\n",
    " - С помощью функции найдите минимальное значение функции Розенброка $$f_2(x)=(1-x_1)^{2}+100(x_2-x_1^{2})^{2}.$$\n",
    "\n",
    " - С помощью функции найдите максимум функции $$f_3(x)=\\frac{10}{30(x_2-x_1^2)^2+5(1.5+x_1)^2+1}.$$\n",
    "\n",
    " - С помощью функции найдите минимальное значение функции  $$f_4(x)=x_1^2+5x_2^2+3x_3^2+4x_1x_2-2x_2x_3-2x_1x_3.$$\n",
    "\n",
    "2) Сравнить результаты п. 1 с результами выполнения заданий 1.1, 1.2, 1.3. Ответ обосновать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8NSJZq-els0"
   },
   "source": [
    "Ваш ответ здесь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "339EXop8epJ4"
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "from scipy.optimize import minimize\n",
    "print(\"Выбрать функцию поиска минимума из библиотеки SciPy\\n\")\n",
    "res = minimize(f2, [3, -2])\n",
    "print(\"x: \" + str([round(x, 3) for x in res.x]), \" f(x): \" + str(round(res.fun, 3)))\n",
    "res = minimize(f3, [1.5, 1.2])\n",
    "print(\"x: \" + str([round(x, 3) for x in res.x]), \" f(x): \" + str(round(res.fun, 3)))\n",
    "res = minimize(f4, [-2.7, 2.27])\n",
    "print(\"x: \" + str([round(x, 3) for x in res.x]), \" f(x): \" + str(-1 * round(res.fun, 3)))\n",
    "res = minimize(f5, [-1.7, -2.3, 3.2])\n",
    "print(\"x: \" + str([round(x, 3) for x in res.x]), \" f(x): \" + str(round(res.fun, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTUhkgIyAR1u"
   },
   "source": [
    "# Часть 2 Решение задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAL_1WNXAZMn"
   },
   "source": [
    "\n",
    "1. Найти и классифицировать матрицу Гессе функции $f\\left(x\\right)=x_{1}^{2} -x_{2}^{2} $\n",
    "2.   Найти и классифицировать матрицу Гессе функции $f\\left(x\\right)=x_{1}^{2} -4x_{1} x_{2}$\n",
    "3.  Методом первого порядка решить задачу $$f(x)=4(x_1-5)^2+(x_2-6)^2\\to \\min, x^0=(8,9)^T, \\varepsilon_1=\\varepsilon_2=0.1.$$\n",
    "4.   Методом первого порядка решить задачу $$f(x)=-x_1x_2e^{-x_1-x_2}\\to \\min, x^0=(0,1)^T, \\varepsilon_1=\\varepsilon_2=0.1.$$\n",
    "5.   Методом второго порядка решить задачу $$f(x)=x_1^2+x_1x_2+2x_2\\to \\min.$$\n",
    "6.   Методом первого порядка решить задачу $$f(x)=-x_1x_2e^{-x_1-x_2}\\to \\min, x^0=(0,1)^T, \\varepsilon_1=\\varepsilon_2=0.1.$$\n",
    "7.   Методом второго порядка решить задачу $$f(x)=x_1^2+3x_1x_2+2x_2\\to \\min.$$\n",
    "\n",
    "8.   Разделить положительное  число на две части так, чтобы произведение произведения этих частей на их разность было максимальным.\n",
    "9.   На плоскости даны три точки: $x_1$, $x_2$, $x_3$. Найти такую точку $x_0$, чтобы сумма квадратов расстояний от $x_0$ до $x_1$, $x_2$, $x_3$ была минимальной.\n",
    "10. Найти безусловный экстремум функции $f(x)=4x^2_1+3x^2_2-4x_1x_2+x_1$.\n",
    "11. Проверить, является ли точка $x^*=(0,0,0)^T$ точкой безусловного экстремума функции $f(x)=x^2_1+2x^2_2-3x^2_3-6x_1x_2+8x_1x_3-4x_2x_3$.\n",
    "12. Проверить, является ли точка $x^*=(1,1)^T$ точкой безусловного экстремума функции $f(x)=(x_2-x_1^2)^2+(1-x_1)^2+10(x^2-1)^2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqJfKswnjSrJ"
   },
   "outputs": [],
   "source": [
    "print(\"№3)  \", gradient_descent_nester_step(lambda x: 4 * (x[0] - 5)**2 + (x[1] - 6)**2,\n",
    "                                   [8, 9], eps1 = 0.1, eps2 = 0.1))\n",
    "print(\"№4,6)  \", gradient_descent_fastest_step(lambda x: -1 * x[0] * x[1] * math.pow(math.e, -1 * x[0] - x[1]),\n",
    "                                    [0, 1], eps1 = 0.1, eps2 = 0.1))\n",
    "print(\"№5)  \", method_newton(lambda x: x[0]**2 + x[0]*x[1] + 2 * x[1], [1, 3]))\n",
    "print(\"№7)  \", method_newton(lambda x: x[0]**2 + 3 * x[0]*x[1] + 2 * x[1], [1, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hASVWpxmC9Am"
   },
   "source": [
    "#Часть 3 Теоретический минимум"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54YYfe8HDCIH"
   },
   "source": [
    "## Теоретический минимум по методам оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgpbgDdtDH7w"
   },
   "source": [
    "1.   В чем заключается задача многомерной безусловной оптимизации?\n",
    "2.   Какие условия называются необходимыми и достаточными условиями безусловного экстремума функции?\n",
    "3.   Что такое точка глобального минимума функции?\n",
    "4.   Что такое точка локального минимума функции?\n",
    "5.   Какая функция называется выпуклой?\n",
    "6.  Что такое матрица Гессе?\n",
    "7.   В чем заключаются принципы построения численных методов поиска безусловного экстремума?\n",
    "8.   Чем характеризуется порядок методов поиска безусловного экстремума?\n",
    "9.   Какой порядок имеет метод градиентного спуска с постоянным шагом?\n",
    "10.  Какой порядок имеет метод наискорейшего градиентного спуска?\n",
    "11.  Какой порядок имеет метод Гаусса-Зейделя?\n",
    "12.  Перечислите методы второго порядка для поиска безусловного экстремума функции нескольких переменных?\n",
    "13.  Какой порядок имеет метод Ньютона?\n",
    "14. Что такое градиент?\n",
    "15.  Какой вид имеет общее правило построения последовательности  ${x^k}$ ? (все переменные пояснить)\n",
    "16.  Каким свойством должны обладать точки последовательности  ${x^k}$ ?\n",
    "17.  Какому условию должно удовлетворять направление спуска  $d^k$  при решении задачи безусловной минимизации?\n",
    "18. Перечислите критерии останова\n",
    "19. Опишите модельную схему решения задач безусловной минимизации\n",
    "20. В чем преимущества метода Ньютона?\n",
    "21. Перечислите недостатки метода Ньютона?\n",
    "22. В чем заключается дополнительное исследование точки $x^k$, полученной методом градиентного спуска с постоянным шагом?\n",
    "23. В чем заключается дополнительное исследование точки $x^k$, полученной методомнаискорейшего градиентного спуска?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPAGLYSE4M_W"
   },
   "source": [
    "Ответы на вопросы:\n",
    "1.\t1. В чем заключается задача многомерной безусловной оптимизации?\n",
    " Задача многомерной безусловной оптимизации заключается в нахождении минимума или максимума функции нескольких переменных без ограничений на эти переменные.\n",
    "2.Какие условия называются необходимыми и достаточными условиями безусловного экстремума функции?\n",
    " Необходимые условия безусловного экстремума - равенство нулю градиента функции, достаточные условия - положительная определенность матрицы Гессе функции.\n",
    "3. Что такое точка глобального минимума функции?\n",
    "Точка глобального минимума функции - это точка, в которой значение функции наименьшее из всех возможных значений на всей области определения функции.\n",
    "4. Что такое точка локального минимума функции?\n",
    " Точка локального минимума функции - это точка, в которой значение функции наименьшее из всех значений в некоторой окрестности этой точки.\n",
    "5. Какая функция называется выпуклой?\n",
    " Функция называется выпуклой, если все точки на отрезке между двумя ее значениями лежат выше или на самой функции.\n",
    "6. Что такое матрица Гессе?\n",
    " Матрица Гессе - это матрица вторых производных функции.\n",
    "7. В чем заключаются принципы построения численных методов поиска безусловного экстремума?\n",
    " Принципы построения численных методов поиска безусловного экстремума заключаются в выборе направления спуска и шага, с которым будет осуществляться спуск.\n",
    "8. Чем характеризуется порядок методов поиска безусловного экстремума?\n",
    " Порядок методов поиска безусловного экстремума характеризуется скоростью сходимости метода.\n",
    "9. Какой порядок имеет метод градиентного спуска с постоянным шагом?\n",
    "Метод градиентного спуска с постоянным шагом имеет порядок сходимости O(1/k), где k - номер итерации.\n",
    "10. Какой порядок имеет метод наискорейшего градиентного спуска?\n",
    " Метод наискорейшего градиентного спуска имеет порядок сходимости O(1/k^2).\n",
    "11. Какой порядок имеет метод Гаусса-Зейделя?\n",
    " Метод Гаусса-Зейделя не является методом поиска безусловного экстремума.\n",
    "12. Перечислите методы второго порядка для поиска безусловного экстремума функции нескольких переменных?\n",
    "Методы второго порядка для поиска безусловного экстремума функции нескольких переменных - метод Ньютона и метод Хука-Дживса.\n",
    "13. Какой порядок имеет метод Ньютона?\n",
    "Метод Ньютона имеет порядок сходимости O(k^2).\n",
    "14. Что такое градиент?\n",
    " Градиент - это вектор, состоящий из частных производных функции по каждой переменной.\n",
    "15. Какой вид имеет общее правило построения последовательности  𝑥𝑘  ? (все переменные пояснить)\n",
    " Общее правило построения последовательности 𝑥𝑘 - 𝑥𝑘+1 = 𝑥𝑘 + 𝛼𝑘𝑑𝑘, где 𝑑𝑘 - направление спуска, 𝛼𝑘 - длина шага.\n",
    "16. Каким свойством должны обладать точки последовательности  𝑥𝑘  ?\n",
    " Точки последовательности 𝑥𝑘 должны удовлетворять условию монотонной последовательности.\n",
    "17. Какому условию должно удовлетворять направление спуска  𝑑𝑘  при решении задачи безусловной минимизации?\n",
    " Направление спуска 𝑑𝑘 должно удовлетворять условию отрицательной скалярной производной.\n",
    "18. Перечислите критерии останова\n",
    " Критерии останова - достижение заданной точности, превышение заданного числа итераций, достижение определенного значения функции.\n",
    "19. Опишите модельную схему решения задач безусловной минимизации\n",
    " Модельная схема решения задач безусловной минимизации - выбор метода, выбор начальной точки, установление критерия останова, проведение итераций до выполнения критерия останова.\n",
    "20. В чем преимущества метода Ньютона?\n",
    " Преимущества метода Ньютона - быстрая сходимость, возможность использования для поиска максимума и минимума.\n",
    "21. Перечислите недостатки метода Ньютона?\n",
    "Недостатки метода Ньютона - вычислительная сложность, неустойчивость к выбору начальной точки, необходимость вычисления вторых производных.\n",
    "22. В чем заключается дополнительное исследование точки  𝑥𝑘 , полученной методом градиентного спуска с постоянным шагом?\n",
    " Дополнительное исследование точки 𝑥𝑘, полученной методом градиентного спуска с постоянным шагом, заключается в проверке условий достаточности безусловного экстремума функции.\n",
    "23. В чем заключается дополнительное исследование точки  𝑥𝑘 , полученной методомнаискорейшего градиентного спуска?\n",
    " Дополнительное исследование точки 𝑥𝑘, полученной методом наискорейшего градиентного спуска, заключается в поиске новой точки, которая удовлетворяет условиям достаточности безусловного экстремума функции.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNM-2RtIDEhw"
   },
   "source": [
    "## Теоретический минимум python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJkazeO7fLts"
   },
   "source": [
    "1. Опишите реализацию градиента в python\n",
    "2. Как реализована матрица вторых производных в python?\n",
    "3. Опишите реализацию критериев останова?\n",
    "4. Какими свойствами обладают методы из функции, которую Вы выбрали?\n",
    "5. Как проводилось дополнительное исследование точки $x^k$ в python, полученной методомнаискорейшего градиентного спуска?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9cHC0484VFH"
   },
   "source": [
    "1) Опишите реализацию градиента в python\n",
    "\n",
    " Реализация градиента в Python может быть выполнена с помощью библиотеки NumPy. Для этого необходимо определить функцию, которая будет вычислять значение градиента для заданной функции в заданной точке. Далее можно использовать метод оптимизации, который будет использовать этот градиент для поиска минимума функции.\n",
    "2) Как реализована матрица вторых производных в python?\n",
    " Матрица вторых производных может быть реализована в Python с помощью библиотеки SciPy. Для этого необходимо определить функцию, которая будет вычислять значение вторых производных для заданной функции в заданной точке. Далее можно использовать эту матрицу для определения направления движения при поиске минимума функции.\n",
    "3) Опишите реализацию критериев останова?\n",
    " Критерии останова могут быть реализованы в Python с помощью проверки различных условий, таких как достижение определенного значения функции или градиента, достижение максимального числа итераций, или изменение функции или градиента менее чем на заданную величину за несколько итераций подряд.\n",
    "4) Какими свойствами обладают методы из функции, которую Вы выбрали?\n",
    "Методы из функции зависят от конкретной функции, которую Вы выбрали. Например, метод наискорейшего градиентного спуска обладает свойством быстрой сходимости, но может страдать от проблемы \"осцилляций\" при наличии плохо обусловленных областей.\n",
    "5) Как проводилось дополнительное исследование точки  𝑥𝑘  в python, полученной методомнаискорейшего градиентного спуска?\n",
    "Дополнительное исследование точки 𝑥𝑘 может проводиться с помощью вычисления значения функции в этой точке и анализа ее свойств, таких как градиент и матрица вторых производных. Также можно использовать методы оптимизации для поиска минимума функции в этой точке.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
